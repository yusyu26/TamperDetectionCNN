import torch
import yaml
import argparse
from tqdm import tqdm
import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
import os
import shutil
import pandas as pd

from model import SaigenCNN
from balanced_data_loader import create_balanced_data_loaders

def copy_image_to_result(image_path, dest_dir, prediction, confidence, true_label):
    """
    ç”»åƒã‚’é©åˆ‡ãªçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚³ãƒ”ãƒ¼
    """
    if not os.path.exists(image_path):
        print(f"âš ï¸ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_path}")
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«åã«äºˆæ¸¬æƒ…å ±ã‚’è¿½åŠ 
    filename = os.path.basename(image_path)
    name, ext = os.path.splitext(filename)
    
    labels = ['original', 'tampered']
    new_filename = f"{name}_pred{labels[prediction]}_true{labels[true_label]}_conf{confidence:.3f}{ext}"
    
    dest_path = os.path.join(dest_dir, new_filename)
    shutil.copy2(image_path, dest_path)

def classify_evaluate(config):
    """
    ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è©•ä¾¡ã—ã€çµæœåˆ¥ã«ç”»åƒã‚’åˆ†é¡ä¿å­˜
    """
    print(f"ğŸš€ GLCMãªã—è»½é‡CNN - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿åˆ†é¡è©•ä¾¡")
    print(f"=" * 60)
    
    
    device = torch.device("cpu")
    print(f"ğŸ–¥ï¸ ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")

    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å–å¾—
    _, _, test_loader = create_balanced_data_loaders(config)
    print(f"ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_loader.dataset)} æš")

    # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
    model = SaigenCNN(
        in_channels=config['model']['in_channels'],
        num_classes=config['model']['num_classes']
    ).to(device)

    model_path = config['training']['model_save_path']
    try:
        model.load_state_dict(torch.load(model_path, map_location=device))
        print(f"âœ… ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {model_path}")
    except FileNotFoundError:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« '{model_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return
    except RuntimeError as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        print(e)
        return

    # è©•ä¾¡å®Ÿè¡Œ
    model.eval()
    all_results = []
    
    print(f"\nğŸ” ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è©•ä¾¡ä¸­...")
    
    with torch.no_grad():
        for batch_idx, (inputs, labels) in enumerate(tqdm(test_loader, desc="åˆ†é¡è©•ä¾¡")):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            
            # ç¢ºç‡ã¨äºˆæ¸¬è¨ˆç®—
            probabilities = torch.softmax(outputs, dim=1)
            confidences, predictions = torch.max(probabilities, 1)
            
            # ãƒãƒƒãƒå†…ã®å„ç”»åƒã«ã¤ã„ã¦å‡¦ç†
            for i in range(inputs.size(0)):
                true_label = labels[i].item()
                predicted_label = predictions[i].item()
                confidence = confidences[i].item()
                
                # ç”»åƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—ï¼ˆå®Ÿéš›ã®ç”»åƒãƒ‘ã‚¹å–å¾—ã®ãŸã‚ï¼‰
                image_idx = batch_idx * test_loader.batch_size + i
                
                # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
                try:
                    # test_loaderã®datasetã‹ã‚‰ç”»åƒãƒ‘ã‚¹ã‚’å–å¾—
                    if hasattr(test_loader.dataset, 'image_paths'):
                        image_path = test_loader.dataset.image_paths[image_idx]
                    elif hasattr(test_loader.dataset, 'samples'):
                        image_path = test_loader.dataset.samples[image_idx][0]
                    else:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®ä»®æƒ³ãƒ‘ã‚¹
                        image_path = f"test_image_{image_idx:04d}.jpg"
                        print(f"âš ï¸ å®Ÿéš›ã®ãƒ‘ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“ã€‚ä»®æƒ³ãƒ‘ã‚¹ä½¿ç”¨: {image_path}")
                        continue
                        
                except IndexError:
                    print(f"âš ï¸ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {image_idx} ã®ç”»åƒãƒ‘ã‚¹ãŒå–å¾—ã§ãã¾ã›ã‚“")
                    continue
                
                # çµæœè¨˜éŒ²
                result = {
                    'image_path': image_path,
                    'true_label': true_label,
                    'predicted_label': predicted_label,
                    'confidence': confidence,
                    'correct': true_label == predicted_label
                }
                all_results.append(result)
                
                # åˆ†é¡ã«åŸºã¥ã„ã¦ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
                if true_label == predicted_label:  # æ­£è§£
                    if true_label == 0:  # æ­£ã—ãã‚ªãƒªã‚¸ãƒŠãƒ«åˆ¤å®š
                        dest_dir = "result/correct/correct_original"
                    else:  # æ­£ã—ãæ”¹ã–ã‚“åˆ¤å®š
                        dest_dir = "result/correct/correct_tampered"
                else:  # ä¸æ­£è§£
                    if predicted_label == 1:  # ã‚ªãƒªã‚¸ãƒŠãƒ«â†’æ”¹ã–ã‚“èª¤åˆ¤å®š
                        dest_dir = "result/incorrect/false_positive"
                    else:  # æ”¹ã–ã‚“â†’ã‚ªãƒªã‚¸ãƒŠãƒ«èª¤åˆ¤å®š
                        dest_dir = "result/incorrect/false_negative"
                
                # ç”»åƒã‚³ãƒ”ãƒ¼å®Ÿè¡Œ
                copy_image_to_result(image_path, dest_dir, predicted_label, confidence, true_label)

    # çµæœåˆ†æ
    df_results = pd.DataFrame(all_results)
    
    # çµ±è¨ˆè¨ˆç®—
    total_images = len(all_results)
    correct_predictions = len(df_results[df_results['correct'] == True])
    accuracy = correct_predictions / total_images if total_images > 0 else 0
    
    # è©³ç´°çµ±è¨ˆ
    correct_original = len(df_results[(df_results['true_label'] == 0) & (df_results['predicted_label'] == 0)])
    correct_tampered = len(df_results[(df_results['true_label'] == 1) & (df_results['predicted_label'] == 1)])
    false_positive = len(df_results[(df_results['true_label'] == 0) & (df_results['predicted_label'] == 1)])
    false_negative = len(df_results[(df_results['true_label'] == 1) & (df_results['predicted_label'] == 0)])
    
    print(f"\nğŸ“Š åˆ†é¡çµæœçµ±è¨ˆ:")
    print(f"   ç·ç”»åƒæ•°: {total_images}")
    print(f"   ç·åˆç²¾åº¦: {accuracy * 100:.2f}%")
    print(f"")
    print(f"   âœ… æ­£è§£åˆ†é¡:")
    print(f"      æ­£ã—ãã‚ªãƒªã‚¸ãƒŠãƒ«åˆ¤å®š: {correct_original} æš")
    print(f"      æ­£ã—ãæ”¹ã–ã‚“åˆ¤å®š: {correct_tampered} æš")
    print(f"   âŒ ä¸æ­£è§£åˆ†é¡:")
    print(f"      èª¤æ”¹ã–ã‚“åˆ¤å®š (False Positive): {false_positive} æš")
    print(f"      æ”¹ã–ã‚“è¦‹é€ƒã— (False Negative): {false_negative} æš")
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåˆ¥ãƒ•ã‚¡ã‚¤ãƒ«æ•°ç¢ºèª
    print(f"\nğŸ“ çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹:")
    result_dirs = {
        "correct/correct_original": correct_original,
        "correct/correct_tampered": correct_tampered,
        "incorrect/false_positive": false_positive,
        "incorrect/false_negative": false_negative
    }
    
    for dir_name, expected_count in result_dirs.items():
        full_path = f"result/{dir_name}"
        actual_count = len([f for f in os.listdir(full_path) if f.lower().endswith(('.jpg', '.jpeg', '.png', '.tif'))])
        status = "âœ…" if actual_count == expected_count else "âš ï¸"
        print(f"   {status} {dir_name}: {actual_count} æš")
    
    # è©³ç´°çµæœã‚’CSVã§ä¿å­˜
    csv_path = "result/detailed_results.csv"
    df_results.to_csv(csv_path, index=False)
    print(f"\nğŸ’¾ è©³ç´°çµæœã‚’ä¿å­˜: {csv_path}")
    
    # æ··åŒè¡Œåˆ—ã‚‚ä¿å­˜
    all_labels = [r['true_label'] for r in all_results]
    all_preds = [r['predicted_label'] for r in all_results]
    
    class_names = ['Original', 'Tampered']
    print(f"\nğŸ“‹ åˆ†é¡ãƒ¬ãƒãƒ¼ãƒˆ:")
    print(classification_report(all_labels, all_preds, target_names=class_names, zero_division=0))

    cm = confusion_matrix(all_labels, all_preds)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=class_names, yticklabels=class_names,
                cbar_kws={'label': 'ç”»åƒæ•°'})
    plt.title('Confusion Matrix')
    plt.ylabel('Actual Label')
    plt.xlabel('Predicted Label')

    plot_path = 'result/confusion_matrix_classification.png'
    plt.savefig(plot_path, dpi=300, bbox_inches='tight')
    plt.show()
    print(f"ğŸ“ˆ æ··åŒè¡Œåˆ—ã‚’ä¿å­˜: {plot_path}")
    
    # ä¿¡é ¼åº¦åˆ†æ
    print(f"\nğŸ” ä¿¡é ¼åº¦åˆ†æ:")
    confidences = [r['confidence'] for r in all_results]
    correct_confidences = [r['confidence'] for r in all_results if r['correct']]
    incorrect_confidences = [r['confidence'] for r in all_results if not r['correct']]
    
    print(f"   å…¨ä½“å¹³å‡ä¿¡é ¼åº¦: {np.mean(confidences):.3f}")
    print(f"   æ­£è§£æ™‚å¹³å‡ä¿¡é ¼åº¦: {np.mean(correct_confidences):.3f}")
    print(f"   ä¸æ­£è§£æ™‚å¹³å‡ä¿¡é ¼åº¦: {np.mean(incorrect_confidences):.3f}")
    
    return df_results

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description="ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†é¡è©•ä¾¡ã¨ç”»åƒåˆ†é¡")
    parser.add_argument('--config', type=str, required=True, help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (config.yaml)")
    args = parser.parse_args()

    with open(args.config, 'r') as f:
        config = yaml.safe_load(f)

    results = classify_evaluate(config)