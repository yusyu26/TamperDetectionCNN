#!/usr/bin/env python3
"""
90%ç²¾åº¦é”æˆãƒ¢ãƒ‡ãƒ«ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
23ãƒãƒ£ãƒ³ãƒãƒ«ç‰¹å¾´é‡ã«ã‚ˆã‚‹ç”»åƒæ”¹ã–ã‚“æ¤œå‡º
"""

import os
import glob
import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
import cv2
import random
import yaml
from typing import List, Tuple, Dict, Any
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

from utils_90percent import create_23_channel_features, normalize_image


class Saigen90Dataset(Dataset):
    """90%ç²¾åº¦å†ç¾ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ23ãƒãƒ£ãƒ³ãƒãƒ«ç‰¹å¾´é‡ï¼‰"""
    
    def __init__(self, image_paths: List[str], labels: List[int], config: Dict[str, Any], mode: str = 'train'):
        """
        Args:
            image_paths: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            labels: ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆ0: ã‚ªãƒªã‚¸ãƒŠãƒ«, 1: æ”¹ã–ã‚“ï¼‰
            config: è¨­å®šè¾æ›¸
            mode: 'train', 'val', 'test'ã®ã„ãšã‚Œã‹
        """
        self.image_paths = image_paths
        self.labels = labels
        self.config = config
        self.mode = mode
        
        # è¨­å®šæŠ½å‡º
        self.image_size = tuple(config['dataset']['image_size'])
        self.preprocessing_config = config['preprocessing']
        self.augmentation_config = config.get('augmentation', {})
        
        print(f"ğŸ“Š Dataset[{mode}]: {len(self.image_paths)} samples")
        print(f"   ã‚ªãƒªã‚¸ãƒŠãƒ«: {labels.count(0)} / æ”¹ã–ã‚“: {labels.count(1)}")
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        ãƒ‡ãƒ¼ã‚¿å–å¾—
        
        Returns:
            features: 23ãƒãƒ£ãƒ³ãƒãƒ«ç‰¹å¾´é‡ (23, H, W)
            label: ãƒ©ãƒ™ãƒ« (scalar)
        """
        # ç”»åƒèª­ã¿è¾¼ã¿
        image_path = self.image_paths[idx]
        image = cv2.imread(image_path)
        
        if image is None:
            raise RuntimeError(f"ç”»åƒèª­ã¿è¾¼ã¿å¤±æ•—: {image_path}")
        
        # BGR â†’ RGBå¤‰æ›
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # ãƒªã‚µã‚¤ã‚º
        image = cv2.resize(image, self.image_size)
        
        # ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µï¼ˆè¨“ç·´æ™‚ã®ã¿ï¼‰
        if self.mode == 'train':
            image = self._apply_data_augmentation(image)
        
        # 23ãƒãƒ£ãƒ³ãƒãƒ«ç‰¹å¾´é‡æŠ½å‡º
        features = create_23_channel_features(image, self.config)
        
        # æ­£è¦åŒ–
        if self.preprocessing_config.get('normalize', True):
            features = normalize_image(features)
        
        # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ› (H, W, C) â†’ (C, H, W)
        features = torch.from_numpy(features).float().permute(2, 0, 1)
        label = torch.tensor(self.labels[idx], dtype=torch.long)
        
        return features, label
    
    def _apply_data_augmentation(self, image: np.ndarray) -> np.ndarray:
        """
        ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®é©ç”¨ï¼ˆæ”¹ã–ã‚“ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è€ƒæ…®ã—ã¦æ§ãˆã‚ï¼‰
        
        Args:
            image: RGBç”»åƒ (H, W, 3)
        
        Returns:
            æ‹¡å¼µæ¸ˆã¿ç”»åƒ (H, W, 3)
        """
        # æ°´å¹³åè»¢
        horizontal_flip = self.augmentation_config.get('horizontal_flip', 0)
        if horizontal_flip > 0 and random.random() < horizontal_flip:
            image = cv2.flip(image, 1)
        
        # å›è»¢
        rotation_range = self.augmentation_config.get('rotation_range', 0)
        if rotation_range > 0:
            angle = random.uniform(-rotation_range, rotation_range)
            center = (image.shape[1] // 2, image.shape[0] // 2)
            matrix = cv2.getRotationMatrix2D(center, angle, 1.0)
            image = cv2.warpAffine(image, matrix, (image.shape[1], image.shape[0]), borderMode=cv2.BORDER_REFLECT)
        
        # æ˜åº¦èª¿æ•´
        brightness_range = self.augmentation_config.get('brightness_range', 0)
        if brightness_range > 0:
            brightness_factor = 1.0 + random.uniform(-brightness_range, brightness_range)
            image = np.clip(image * brightness_factor, 0, 255).astype(np.uint8)
        
        # ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆèª¿æ•´
        contrast_range = self.augmentation_config.get('contrast_range', 0)
        if contrast_range > 0:
            contrast_factor = 1.0 + random.uniform(-contrast_range, contrast_range)
            image = np.clip((image - 128) * contrast_factor + 128, 0, 255).astype(np.uint8)
        
        return image


def load_dataset_paths(config: Dict[str, Any]) -> Tuple[List[str], List[int]]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ‘ã‚¹ã¨ãƒ©ãƒ™ãƒ«ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        config: è¨­å®šè¾æ›¸
    
    Returns:
        image_paths: ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        labels: ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆï¼ˆ0: ã‚ªãƒªã‚¸ãƒŠãƒ«, 1: æ”¹ã–ã‚“ï¼‰
    """
    dataset_config = config['dataset']
    data_path = dataset_config['data_path']
    authentic_folder = dataset_config['authentic_folder']
    tampered_folder = dataset_config['tampered_folder']
    max_samples = dataset_config.get('max_samples_per_class', None)
    
    image_paths = []
    labels = []
    
    # ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒï¼ˆãƒ©ãƒ™ãƒ«0ï¼‰
    authentic_path = os.path.join(data_path, authentic_folder)
    authentic_files = glob.glob(os.path.join(authentic_path, "*.jpg"))
    
    if max_samples:
        authentic_files = authentic_files[:max_samples]
    
    image_paths.extend(authentic_files)
    labels.extend([0] * len(authentic_files))
    
    # æ”¹ã–ã‚“ç”»åƒï¼ˆãƒ©ãƒ™ãƒ«1ï¼‰
    tampered_path = os.path.join(data_path, tampered_folder)
    tampered_files = glob.glob(os.path.join(tampered_path, "*.jpg"))
    
    if max_samples:
        tampered_files = tampered_files[:max_samples]
    
    image_paths.extend(tampered_files)
    labels.extend([1] * len(tampered_files))
    
    print(f"ğŸ“ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿å®Œäº†:")
    print(f"   ã‚ªãƒªã‚¸ãƒŠãƒ«: {len(authentic_files)} æš")
    print(f"   æ”¹ã–ã‚“: {len(tampered_files)} æš")
    print(f"   åˆè¨ˆ: {len(image_paths)} æš")
    
    return image_paths, labels


def create_data_splits(image_paths: List[str], labels: List[int], config: Dict[str, Any]) -> Tuple[
    Tuple[List[str], List[int]], 
    Tuple[List[str], List[int]], 
    Tuple[List[str], List[int]]
]:
    """
    ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è¨“ç·´ãƒ»æ¤œè¨¼ãƒ»ãƒ†ã‚¹ãƒˆã«åˆ†å‰²
    
    Args:
        image_paths: ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        labels: ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ
        config: è¨­å®šè¾æ›¸
    
    Returns:
        (train_paths, train_labels): è¨“ç·´ãƒ‡ãƒ¼ã‚¿
        (val_paths, val_labels): æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿
        (test_paths, test_labels): ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    """
    dataset_config = config['dataset']
    train_ratio = dataset_config['train_ratio']
    val_ratio = dataset_config['val_ratio']
    test_ratio = dataset_config['test_ratio']
    seed = dataset_config['seed']
    
    # æ¯”ç‡ã®ç¢ºèª
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, "åˆ†å‰²æ¯”ç‡ã®åˆè¨ˆãŒ1ã«ãªã‚Šã¾ã›ã‚“"
    
    # ã¾ãšè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    train_val_paths, test_paths, train_val_labels, test_labels = train_test_split(
        image_paths, labels, test_size=test_ratio, 
        random_state=seed, stratify=labels
    )
    
    # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã«åˆ†å‰²
    adjusted_val_ratio = val_ratio / (train_ratio + val_ratio)
    train_paths, val_paths, train_labels, val_labels = train_test_split(
        train_val_paths, train_val_labels, test_size=adjusted_val_ratio,
        random_state=seed, stratify=train_val_labels
    )
    
    print(f"ğŸ”€ ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†:")
    print(f"   è¨“ç·´: {len(train_paths)} æš ({len(train_paths)/len(image_paths)*100:.1f}%)")
    print(f"   æ¤œè¨¼: {len(val_paths)} æš ({len(val_paths)/len(image_paths)*100:.1f}%)")
    print(f"   ãƒ†ã‚¹ãƒˆ: {len(test_paths)} æš ({len(test_paths)/len(image_paths)*100:.1f}%)")
    
    return (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels)


def create_saigen90_data_loaders(config: Dict[str, Any]) -> Tuple[DataLoader, DataLoader, DataLoader]:
    """
    90%ç²¾åº¦å†ç¾ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
    
    Args:
        config: è¨­å®šè¾æ›¸
    
    Returns:
        train_loader: è¨“ç·´ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        val_loader: æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        test_loader: ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
    image_paths, labels = load_dataset_paths(config)
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    (train_paths, train_labels), (val_paths, val_labels), (test_paths, test_labels) = create_data_splits(
        image_paths, labels, config
    )
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    train_dataset = Saigen90Dataset(train_paths, train_labels, config, mode='train')
    val_dataset = Saigen90Dataset(val_paths, val_labels, config, mode='val')
    test_dataset = Saigen90Dataset(test_paths, test_labels, config, mode='test')
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆï¼ˆDockerç’°å¢ƒå¯¾å¿œï¼‰
    training_config = config['training']
    batch_size = training_config['batch_size']
    num_workers = 0  # Dockerç’°å¢ƒã§ã¯0ã«å›ºå®š
    pin_memory = False  # Dockerç’°å¢ƒã§ã¯ç„¡åŠ¹åŒ–
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True
    )
    
    val_loader = DataLoader(
        val_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    print(f"ğŸš€ ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆå®Œäº†:")
    print(f"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
    print(f"   ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°: {num_workers}")
    print(f"   è¨“ç·´ãƒãƒƒãƒæ•°: {len(train_loader)}")
    print(f"   æ¤œè¨¼ãƒãƒƒãƒæ•°: {len(val_loader)}")
    print(f"   ãƒ†ã‚¹ãƒˆãƒãƒƒãƒæ•°: {len(test_loader)}")
    
    return train_loader, val_loader, test_loader


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    print("=" * 60)
    print("Saigen90 ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆ")
    print("=" * 60)
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    with open('config_90percent.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    try:
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_loader, val_loader, test_loader = create_saigen90_data_loaders(config)
        
        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        print("\nğŸ” ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª:")
        for batch_idx, (features, labels) in enumerate(train_loader):
            print(f"   ãƒãƒƒãƒ {batch_idx + 1}:")
            print(f"     ç‰¹å¾´é‡å½¢çŠ¶: {features.shape}")  # (B, 23, H, W)
            print(f"     ãƒ©ãƒ™ãƒ«å½¢çŠ¶: {labels.shape}")    # (B,)
            print(f"     ç‰¹å¾´é‡ç¯„å›²: [{features.min():.3f}, {features.max():.3f}]")
            print(f"     ãƒ©ãƒ™ãƒ«: {labels.tolist()}")
            
            if batch_idx >= 2:  # æœ€åˆã®3ãƒãƒƒãƒã®ã¿ç¢ºèª
                break
        
        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ãƒ†ã‚¹ãƒˆæˆåŠŸ!")
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
